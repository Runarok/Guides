{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Basics of Calling Google Gemini Model and LangChain (LCEL)"
      ],
      "metadata": {
        "id": "xPRVq3e03c1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Gemini API Setup: Installation and configuration of Gemini API credentials with secure\n",
        "# environment variable management for production-ready access to Google's language models.\n",
        "\n",
        "# Direct Model Invocation: Implementation of the GenerativeModel class for making basic text\n",
        "# generation calls to Gemini models, establishing the foundation for understanding API interaction\n",
        "# patterns.\n",
        "\n",
        "# LangChain Integration: Adoption of the ChatGoogleGenerativeAI abstraction layer to create a\n",
        "# consistent interface for prompt management across different LLM providers.\n",
        "\n",
        "# Prompt Templates: Development of reusable templates with variable placeholders that allow for\n",
        "# standardized yet flexible interactions with language models.\n",
        "\n",
        "# LCEL (LangChain Expression Language): Utilization of the chain composition pattern with pipe\n",
        "# operators to build clean, modular processing pipelines for language model interactions.\n",
        "\n",
        "# Batch Processing: Implementation of the map() method for parallel processing of multiple prompts,\n",
        "# significantly improving throughput for batch operations.\n",
        "\n",
        "# Complex Prompting: Creation of multi-variable templates that enable precise control over prompt\n",
        "# construction while accommodating varied contextual parameters."
      ],
      "metadata": {
        "id": "D796tZFtOtco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install App and LLM dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -q\n",
        "!pip install langchain-google-genai -q\n",
        "!pip install langchain-community -q\n",
        "!pip install google-generativeai -q"
      ],
      "metadata": {
        "id": "2evPp14fy258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3f0497-44df-434f-c5e8-15f8984efce8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m694.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-google-genai 2.1.4 requires google-ai-generativelanguage<0.7.0,>=0.6.18, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Gemini API Credentials\n",
        "\n",
        "Here we load it from the secret key so we don't explore the credentials on the internet by mistake"
      ],
      "metadata": {
        "id": "CiwGjVWK4q6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set your API key directly here (replace 'your_api_key' with your actual API key)\n",
        "os.environ[\"GOOGLE_API_KEY\"] = 'AIzaSyDNrRLopzwFnEMLdNGBAI9hDpsTuLQVWNs'\n",
        "\n",
        "# Configure the API key\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ],
      "metadata": {
        "id": "a6OLI6AtKfcX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "# # Configure API key\n",
        "# genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ],
      "metadata": {
        "id": "MlBQh5fdkPPY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash-lite-001\")"
      ],
      "metadata": {
        "id": "3Pswa8xvcgH-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "response = gemini_model.generate_content(\"What is the capital of France?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "reAkGri7cjXa",
        "outputId": "b3738cf2-556a-4ea4-b754-a4e12990d322"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is **Paris**.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbSFoz3Lp0pJ",
        "outputId": "5b7b7d27-ff87-436c-aa69-f79f3ec0710d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "response:\n",
              "GenerateContentResponse(\n",
              "    done=True,\n",
              "    iterator=None,\n",
              "    result=protos.GenerateContentResponse({\n",
              "      \"candidates\": [\n",
              "        {\n",
              "          \"content\": {\n",
              "            \"parts\": [\n",
              "              {\n",
              "                \"text\": \"The capital of France is **Paris**.\\n\"\n",
              "              }\n",
              "            ],\n",
              "            \"role\": \"model\"\n",
              "          },\n",
              "          \"finish_reason\": \"STOP\",\n",
              "          \"avg_logprobs\": -0.01782448258664873\n",
              "        }\n",
              "      ],\n",
              "      \"usage_metadata\": {\n",
              "        \"prompt_token_count\": 7,\n",
              "        \"candidates_token_count\": 9,\n",
              "        \"total_token_count\": 16\n",
              "      },\n",
              "      \"model_version\": \"gemini-2.0-flash-lite-001\"\n",
              "    }),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"gemini-2.0-flash-lite-001\"\n",
        "\n",
        "gemini_model = genai.GenerativeModel(model_id)"
      ],
      "metadata": {
        "id": "Vv2RMwSSdT8b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the capital of India?\"\n",
        "\n",
        "response = gemini_model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "fCq3kHzFdhVF",
        "outputId": "8d3bc0a0-76e7-43a6-d4b1-152ef9fe30b5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of India is **New Delhi**.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Direct Model Invocation:\n",
        "# Next, we’re creating a function that can easily generate responses using Google Gemini. You define the prompt, and the function will generate content based on the question you ask."
      ],
      "metadata": {
        "id": "E3WpExZ5Q76j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(prompt):\n",
        "  gemini_model = genai.GenerativeModel(model_id)\n",
        "  response = gemini_model.generate_content(prompt)\n",
        "  return response.text"
      ],
      "metadata": {
        "id": "a48DyrIXrB1n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt1 = \"Coder\"\n",
        "system_prompt2 = \"Reviewer\"\n",
        "\n",
        "user_query = \"develop a web page for food company\""
      ],
      "metadata": {
        "id": "fE2QC6To2c3T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_response(\"What is the capital of India?\")"
      ],
      "metadata": {
        "id": "qz7ZXwzUPiZU",
        "outputId": "18abea7a-5c6e-40c0-f96a-b5a61fe61f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of India is **New Delhi**.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. LangChain Integration:\n",
        "# Now, we bring in LangChain to make things even easier for managing prompts across different LLM providers (like Gemini, OpenAI, etc.)."
      ],
      "metadata": {
        "id": "Mw1HCIAAREo6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "model = ChatGoogleGenerativeAI(model=model_id,\n",
        "                                      convert_system_message_to_human=True)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a joke about GenAI\")\n",
        "\n",
        "chain = (\n",
        "         prompt\n",
        "         |\n",
        "         model\n",
        ")\n",
        "\n",
        "response = chain.invoke({})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvLbyyiDeUv_",
        "outputId": "3ef3a4df-63fe-418b-998e-dfc507e8acb0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:390: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the GenAI refuse to write a joke?\n",
            "\n",
            "Because it was afraid it would be *too* original and put all the human comedians out of a job! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LangChain's Role: LangChain provides an abstraction layer for working with different language models, making it easier to switch between providers.\n",
        "# In this case, you're using the ChatGoogleGenerativeAI to interface with Gemini.\n",
        "\n",
        "# What’s this?: This is where LangChain shines. The ChatPromptTemplate creates a template for your prompt. You can easily substitute values and variables like {topic} later,\n",
        "# making the prompt flexible.\n",
        "# The chain operator (|) links the prompt template with the Gemini model, forming a processing pipeline.\n",
        "# Finally, chain.invoke({}) sends the prompt to the model and gets a response. This response is printed, and it will probably be a joke about Generative AI."
      ],
      "metadata": {
        "id": "tWYbH3OaRgyN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Prompt Templates:\n",
        "# Now, let’s make our prompts dynamic. We want to make a prompt that can change based on input values.\n",
        "\n",
        "\n",
        "# Dynamic Prompts: Notice how the {topic} placeholder is used in the prompt. With LangChain, you can dynamically substitute values like \"GenAI\" or \"Mumbai\" using a\n",
        "# dictionary passed to invoke(). This keeps your prompts flexible."
      ],
      "metadata": {
        "id": "swsjtHvMRvE7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "model = ChatGoogleGenerativeAI(model=model_id,\n",
        "                                      convert_system_message_to_human=True)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
        "\n",
        "chain = (\n",
        "         prompt\n",
        "         |\n",
        "         model\n",
        ")\n",
        "\n",
        "response = chain.invoke({\"topic\": \"GenAI\"})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvEUaoxxmq5Z",
        "outputId": "486dac2b-9df6-44bb-be14-4cd4f7ac31e6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:390: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the GenAI bot refuse to write a joke?\n",
            "\n",
            "Because it was afraid it would be too... **algorithmically funny!** \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Batch Processing:\n",
        "# Now, we're adding batch processing. This is like sending multiple questions to the model at once — parallel processing to handle many tasks simultaneously.\n",
        "\n",
        "\n",
        "# What’s happening here?: The map() function allows us to send multiple prompts to the model simultaneously. Each prompt has a different topic (GenAI and Mumbai).\n",
        "# The model will generate responses for both, and we handle them all at once.\n"
      ],
      "metadata": {
        "id": "SBWZt83bSIxP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGoogleGenerativeAI(model=model_id,convert_system_message_to_human=True)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
        "\n",
        "chain = (\n",
        "         prompt\n",
        "         |\n",
        "         model\n",
        ")\n",
        "\n",
        "responses = chain.map().invoke([{\"topic\":\"GenAI\"},{\"topic\":\"Mumbai\"}])"
      ],
      "metadata": {
        "id": "JRUMbtFSf7pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0b89a8-67cc-4e41-bf97-340fbbf3bb6b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:390: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:390: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "responses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3I18ndtqHQ-",
        "outputId": "6ee63b01-fd0c-4616-c7df-761ad08f83c8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='Why did the GenAI refuse to write a poem about a cat?\\n\\nBecause it kept generating purr-fectly terrible puns!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite-001', 'safety_ratings': []}, id='run-4155e0a6-8462-438b-a0fc-ac3274430229-0'),\n",
              " AIMessage(content='Why did the Mumbai traffic jam take so long?\\n\\nBecause everyone was trying to get to the Gateway of India... and it was a Tuesday. ', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite-001', 'safety_ratings': []}, id='run-dda113d1-19e3-4869-b776-bde1ce33d2fb-0')]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for response in responses:\n",
        "  print(response.content)\n",
        "  print(\"----------------------------------\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h_3m_9GqXpR",
        "outputId": "780a5d0f-1d1a-440f-cf48-432a45699b53"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the GenAI refuse to write a poem about a cat?\n",
            "\n",
            "Because it kept generating purr-fectly terrible puns!\n",
            "----------------------------------\n",
            "\n",
            "\n",
            "Why did the Mumbai traffic jam take so long?\n",
            "\n",
            "Because everyone was trying to get to the Gateway of India... and it was a Tuesday. \n",
            "----------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Complex prompts with placeholders"
      ],
      "metadata": {
        "id": "xV-cqRgBq33i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Complex Prompting:\n",
        "# Now we step it up a notch. Imagine asking for a detailed explanation of something — but you want it in a specific way for different audiences."
      ],
      "metadata": {
        "id": "6hj_sN2BSR6O"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Prompts: We use input data (like \"Generative AI\" for kids or \"Quantum Physics\" for GenZ adults) to generate custom prompts for each scenario. You can imagine this as asking the AI to \"explain quantum physics to a child\" or \"recommendation engines to seniors.\""
      ],
      "metadata": {
        "id": "2ymAmTqfSaOY"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template using ChatPromptTemplate.from_template()\n",
        "prompt_template = ChatPromptTemplate.from_template(\n",
        "    \"Explain to me what is {topic} in 500 words like you would do to a {audience}?\"\n",
        ")\n",
        "\n",
        "# Your input data\n",
        "input_data = [\n",
        "    {\"topic\": \"Generative AI\", \"audience\": \"Child\"},\n",
        "    {\"topic\": \"Recommendation Engine\", \"audience\": \"Senior Citizen\"},\n",
        "    {\"topic\": \"Quantum Physics\", \"audience\": \"GenZ Adult\"}\n",
        "]\n",
        "\n",
        "# Generate prompts for each input using list comprehension:\n",
        "prompts = [prompt_template.format_messages(topic=data[\"topic\"], audience=data[\"audience\"]) for data in input_data]\n",
        "\n",
        "# Generate prompts for each input using for loop\n",
        "# prompts = []\n",
        "# for data in input_data:\n",
        "#     prompt = prompt_template.format_messages(topic=data[\"topic\"], audience=data[\"audience\"])\n",
        "#     prompts.append(prompt)\n",
        "\n",
        "# Display the prompts\n",
        "for prompt in prompts:\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpfAbKWiqptc",
        "outputId": "af6281a9-cab7-4aab-f253-8732b1ed47bf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: [HumanMessage(content='Explain to me what is Generative AI in 500 words like you would do to a Child?', additional_kwargs={}, response_metadata={})]\n",
            "--------------------------------------------------\n",
            "Prompt: [HumanMessage(content='Explain to me what is Recommendation Engine in 500 words like you would do to a Senior Citizen?', additional_kwargs={}, response_metadata={})]\n",
            "--------------------------------------------------\n",
            "Prompt: [HumanMessage(content='Explain to me what is Quantum Physics in 500 words like you would do to a GenZ Adult?', additional_kwargs={}, response_metadata={})]\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "responses = chain.map().invoke(prompts)"
      ],
      "metadata": {
        "id": "IPEcB974s3RV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a92187-6fbe-4bd3-e685-433fa14d27bb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:390: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:390: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:390: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for response in responses:\n",
        "  print(response.content)\n",
        "  print(\"-\" * 50)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4wJpezltINJ",
        "outputId": "789850fa-d58a-48d3-eb3c-f9a429759acc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here's a joke about Generative AI, written so a child can understand it, and then I'll explain Generative AI in a child-friendly way:\n",
            "\n",
            "**The Joke:**\n",
            "\n",
            "Why did the Generative AI get a tummy ache?\n",
            "\n",
            "...Because it ate *too many* pictures of kittens! It was trying to *make* its own kitten, but it got confused and made a fluffy, purple… *thing!*\n",
            "\n",
            "---\n",
            "\n",
            "**Now, let's talk about Generative AI! Imagine it like this:**\n",
            "\n",
            "Imagine you have a super-duper smart robot, like a really, really cool friend. This robot's name is **Generative AI**.\n",
            "\n",
            "Now, this robot is AMAZING! It can learn about things by looking at lots and lots of examples. Think of it like this:\n",
            "\n",
            "*   **Learning about animals:** You show the robot thousands of pictures of dogs, cats, birds, and fish. The robot studies them all, sees what they have in common (like fur or feathers, eyes, and a nose) and what makes them different.\n",
            "*   **Learning about stories:** You give the robot lots of storybooks to read. It learns what makes a good story – characters, a problem, a solution, and a happy ending!\n",
            "*   **Learning about music:** You let the robot listen to thousands of songs. It learns about different instruments, how the notes sound together, and what makes a song catchy.\n",
            "\n",
            "**The \"Generative\" part is the most important!**\n",
            "\n",
            "Once the robot has *learned* these things, it can do something super cool: **It can *create* new things!**\n",
            "\n",
            "*   **Creating new pictures:** You ask the robot, \"Can you draw a picture of a fluffy cat riding a unicorn?\" And the robot, using what it learned about cats, unicorns, and drawing, *creates* a brand-new picture just for you! It's like magic!\n",
            "*   **Creating new stories:** You tell the robot, \"Write a story about a brave little robot who saves the world.\" And the robot, using what it learned about stories, *writes* a brand-new story, with characters, a problem, and a solution! It's like having your own personal author!\n",
            "*   **Creating new music:** You tell the robot, \"Make a song that sounds like a happy, bouncy dance tune.\" And the robot, using what it learned about music, *creates* a brand-new song for you to dance to! It's like having your own personal composer!\n",
            "\n",
            "**So, Generative AI is like a super-smart robot that learns from examples and then can *create* new things based on what it learned.** It's like a magical art, story, and music-making machine! It's still learning and getting better all the time, just like you! Sometimes it makes silly things, like the purple fluffy thing in the joke, but it's always fun to see what it comes up with!\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "Alright, settle in, friend. Let me tell you about these “Recommendation Engines.” Think of them like your friendly neighbor, Mrs. Gable, who always knows what you like.\n",
            "\n",
            "Imagine you're going to the grocery store. You always buy your favorite brand of coffee, and you *always* get the sugar-free cookies. Mrs. Gable, knowing you, might say, “Oh, dear, they’re having a sale on your favorite coffee this week, and those new sugar-free biscotti look delicious! You might like those.” That's the basic idea of a recommendation engine.\n",
            "\n",
            "Now, instead of Mrs. Gable, we're talking about computers. These computers are programmed to learn about you. They watch what you do online, what you buy, what you search for, what you click on, and what you tell them you like. They're like a super-powered, information-gathering Mrs. Gable!\n",
            "\n",
            "So, let's say you're shopping online for a new book. A recommendation engine would be looking at a few things:\n",
            "\n",
            "*   **What you've bought before:** If you've bought historical fiction in the past, it might suggest more historical fiction.\n",
            "*   **What other people like you have bought:** It looks at what other people who bought the same books you did *also* bought. Maybe they bought a book by the same author, or a book about a similar historical period. It's like Mrs. Gable saying, \"Oh, you like that author? My friend Mildred loves this other author, and I think you'd enjoy them too!\"\n",
            "*   **What you've searched for:** If you searched for \"gardening tools,\" it might suggest some gardening gloves or a new trowel.\n",
            "*   **What you've clicked on:** If you've browsed a particular section of a website, it might suggest items from that section.\n",
            "\n",
            "The goal is to help you find things you might like, things you might not have discovered on your own. It's about making your life easier, like Mrs. Gable pointing you towards something you’d enjoy.\n",
            "\n",
            "These engines are used everywhere! Think about:\n",
            "\n",
            "*   **Streaming services (like that Netflix you use):** They suggest movies and shows based on what you've watched.\n",
            "*   **Online stores (like Amazon):** They suggest products you might want to buy.\n",
            "*   **News websites:** They suggest articles you might be interested in reading.\n",
            "*   **Even social media:** They suggest friends to connect with or posts to read.\n",
            "\n",
            "The good thing is, the more you use these systems, the better they get at predicting what you want. It's like Mrs. Gable really getting to know you and your preferences! So, next time you see a suggestion online, remember Mrs. Gable and think of it as a friendly nudge in the right direction. Now, who wants a cup of coffee?\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "Alright, buckle up, fam. Quantum Physics, or \"quantum mechanics\" if you're feeling fancy, is basically the ultimate mind-bender of the universe. Imagine reality is a super glitchy video game, and the developers (the universe itself) are constantly updating the code with weird, counter-intuitive stuff. Here's the lowdown:\n",
            "\n",
            "**Forget Everything You Think You Know (About Reality):**\n",
            "\n",
            "*   **Tiny Stuff Matters:** Quantum physics deals with the super-duper tiny, like atoms, electrons, and the even smaller particles that make them up (we're talking smaller than a grain of sand is to a planet). These tiny things don't play by the same rules as the big stuff we see every day.\n",
            "*   **Wave-Particle Duality: They're Kinda Both:** This is the OG mind-melt. Imagine an electron. Is it a tiny ball (particle) or a wave, like the ripple you make in a pool? The answer is... both! Sometimes it acts like a particle, sometimes like a wave. It depends on how you're looking at it. It's like your mood on a Monday – unpredictable.\n",
            "*   **Quantization: The Universe is a Staircase:** Energy, like light or heat, doesn't come in a smooth flow. It comes in discrete packets, like steps on a staircase. These packets are called \"quanta.\" Think of it like buying a song on iTunes – you can't buy half a song, you buy the whole track (quantum of energy).\n",
            "*   **Superposition: The \"Maybe\" State:** Imagine an electron. It can be in multiple places at once, or have multiple possible states (like spinning up or down), until you measure it. Only when you look, does it \"choose\" a single state. It's like your unread texts – you don't know who messaged you yet, until you open the chat.\n",
            "*   **Uncertainty Principle: You Can't Know Everything:** You can't know both the position and the momentum (speed and direction) of a particle with perfect accuracy. The more you know about one, the less you know about the other. It's like trying to focus on your TikTok feed and your homework at the same time - you're bound to miss something.\n",
            "*   **Entanglement: Spooky Action at a Distance:** This is the most bonkers part. Two particles can become linked, no matter how far apart they are. If you measure something about one, you instantly know something about the other, even if they're light-years away. Think of it as a psychic connection between two friends – they finish each other's sentences, even if they're far away.\n",
            "\n",
            "**Why Should You Care?**\n",
            "\n",
            "Quantum physics isn't just for nerds in lab coats. It's the foundation of:\n",
            "\n",
            "*   **Your phone:** Those microchips are quantum-level magic.\n",
            "*   **Lasers:** They use the principles of quantum mechanics.\n",
            "*   **MRI machines:** They use quantum properties to create images of your insides.\n",
            "*   **Future Tech:** Quantum computing could revolutionize everything from medicine to artificial intelligence.\n",
            "\n",
            "So, basically, Quantum Physics is the reason why your phone works, and the reason why the universe is more weird and wonderful than you ever imagined. Now, go forth and impress your friends with your newfound knowledge. And remember, just because you don't understand it completely, doesn't mean it's not real. Now go ask your friends about the joke!\n",
            "--------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nN88RUalLZyr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
