{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LinkedIn Post Generator App\n",
        "\n",
        "Here we will build an AI Application using Gemini, LangChain and Streamlit with the following features:\n",
        "\n",
        "- Custom Landing Page\n",
        "- LinkedIn Post Generation\n",
        "- Streamlit features"
      ],
      "metadata": {
        "id": "xPRVq3e03c1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install App and LLM dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.1.12 -q\n",
        "!pip install langchain-google-genai==0.0.7 -q\n",
        "!pip install langchain-community==0.0.29 -q\n",
        "!pip install streamlit==1.32.2 -q\n",
        "!pip install pyngrok==7.1.5 -q\n",
        "!pip install google-generativeai>=0.3.2 -q"
      ],
      "metadata": {
        "id": "2evPp14fy258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75ace886-4b2e-43a6-ff36-56c43540273c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m916.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.1/303.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "google-cloud-bigquery 3.31.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.9/146.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m598.7/598.7 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "google-cloud-bigquery 3.31.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Gemini API Credentials\n",
        "\n",
        "Here we load it from a file so we don't explore the credentials on the internet by mistake"
      ],
      "metadata": {
        "id": "CiwGjVWK4q6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GOOGLE_API_KEY'] = 'AIzaSyDNrRLopzwFnEMLdNGBAI9hDpsTuLQVWNs'"
      ],
      "metadata": {
        "id": "kDe44J0N0NcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set your API key directly here (replace 'your_api_key' with your actual API key)\n",
        "os.environ[\"GOOGLE_API_KEY\"] = 'AIzaSyDNrRLopzwFnEMLdNGBAI9hDpsTuLQVWNs'\n",
        "\n",
        "# Configure the API key\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ],
      "metadata": {
        "id": "6a2GbhkYTHbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# streamlit as st:\n",
        "# Streamlit is a framework for building interactive web applications. It’s used here to build the app's user interface (UI), handle inputs, and display outputs.\n",
        "# langchain_google_genai:\n",
        "# ChatGoogleGenerativeAI: This is a class that interfaces with Google’s Gemini model via the LangChain framework, allowing you to make AI-powered requests.\n",
        "# langchain.prompts.ChatPromptTemplate:\n",
        "# ChatPromptTemplate: This is a class used to manage the prompt template. It allows you to define how inputs (e.g., user queries) are structured for interaction with language models.\n",
        "# langchain.schema.SystemMessage, HumanMessage:\n",
        "# These are classes used for structuring the conversation's components. SystemMessage is typically used for setting instructions or the context (e.g., guidelines for the AI model), and HumanMessage represents the user's input.\n"
      ],
      "metadata": {
        "id": "8Pmh3OvuT3vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile linkedin_app.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "\n",
        "# st.title(): This function sets the title of the Streamlit app. It will be displayed at the top of the page as \"LinkedIn Post Generator\".\n",
        "st.title(\"LinkedIn Post Generator\")\n",
        "\n",
        "# System prompt that defines the app's behavior\n",
        "system_prompt = \"\"\"You are a professional LinkedIn post generator.\n",
        "Your task is to create engaging, professional posts for LinkedIn based on the topic provided by the user.\n",
        "\n",
        "Follow these guidelines:\n",
        "- Keep posts between 150-300 words\n",
        "- Include relevant hashtags (3-5)\n",
        "- Maintain a professional but conversational tone\n",
        "- Focus on providing value to the reader\n",
        "- Structure posts with short paragraphs for readability\n",
        "- Avoid clickbait and exaggerated claims\n",
        "\n",
        "The user will provide a topic or idea for the post.\"\"\"\n",
        "\n",
        "# Initialize the LLM\n",
        "# gemini:\n",
        "# This is an instance of the ChatGoogleGenerativeAI class. It initializes the connection to the Google Gemini model (with a specific version of the model: gemini-2.0-flash-thinking-exp-01-21).\n",
        "# temperature=0.7: This controls the creativity/randomness of the generated content. A value of 0.7 allows for more creativity.\n",
        "# convert_system_message_to_human=True: This tells LangChain to treat the system message (instructions) as if they were part of the conversation for the human-like interaction.\n",
        "gemini = ChatGoogleGenerativeAI(model='gemini-2.0-flash-thinking-exp-01-21',\n",
        "                               temperature=0.7,\n",
        "                                convert_system_message_to_human=True)\n",
        "\n",
        "# Create the prompt template\n",
        "# prompt_template:\n",
        "# This ChatPromptTemplate is used to define how the input from the user and system instructions are structured.\n",
        "# (\"system\", system_prompt): The system message (which is a set of instructions) is placed first.\n",
        "# (\"human\", \"{user_input}\"): The user's input, provided via the text area, is used in place of the placeholder {user_input}.\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# App description\n",
        "# These st.markdown() statements are used to display the descriptive text on the app interface. They explain what the app does and provide instructions.\n",
        "st.markdown(\"### Generate professional LinkedIn posts with AI\")\n",
        "st.markdown(\"Enter a topic or idea to get a LinkedIn post tailored to your needs.\")\n",
        "\n",
        "# Input area with specific placeholder text\n",
        "# user_input:\n",
        "# This creates a text area for the user to input their topic. It comes with a placeholder text guiding the user on how to phrase the topic (e.g., \"Sharing my thoughts on the future of AI in healthcare\").\n",
        "# The height is set to 100 pixels to provide ample space for the input.\n",
        "user_input = st.text_area(\"What would you like to post about?\",\n",
        "                          placeholder=\"Example: Sharing my thoughts on the future of AI in healthcare\",\n",
        "                          height=100)\n",
        "\n",
        "# Generate button\n",
        "# st.button(\"Generate Post\"):\n",
        "# This adds a button labeled \"Generate Post\". When clicked, it triggers the generation of the LinkedIn post.\n",
        "# if user_input::\n",
        "# Ensures that the user has entered a topic before attempting to generate the post. If no topic is entered, an error message is displayed.\n",
        "# st.spinner(\"Creating your LinkedIn post...\"):\n",
        "# Displays a loading spinner while the AI model is generating the post.\n",
        "# prompt_template.format_messages(user_input=user_input):\n",
        "# This formats the prompt with the user's input, preparing it for the Gemini model.\n",
        "# gemini.invoke(messages):\n",
        "# This calls the invoke() method of the gemini object, sending the prompt to the Gemini model and getting a response.\n",
        "# st.markdown(response.content):\n",
        "# Displays the generated post in a clean format on the UI.\n",
        "if st.button(\"Generate Post\"):\n",
        "    if user_input:\n",
        "        with st.spinner(\"Creating your LinkedIn post...\"):\n",
        "            # Create the prompt with the user's input\n",
        "            messages = prompt_template.format_messages(user_input=user_input)\n",
        "\n",
        "            # Invoke the LLM\n",
        "            response = gemini.invoke(messages)\n",
        "\n",
        "            # Display the result in a nice format\n",
        "            st.markdown(\"### Your LinkedIn Post:\")\n",
        "            st.markdown(response.content)\n",
        "\n",
        "            # Copy button\n",
        "            st.markdown(\"---\")\n",
        "            st.markdown(\"Copy this post to your clipboard and share it on LinkedIn!\")\n",
        "    else:\n",
        "        st.error(\"Please enter a topic for your LinkedIn post.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HSQiIgGTcKg",
        "outputId": "a81097b7-0bb6-4da5-ea8d-50e761a9ab95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing linkedin_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run linkedin_app.py --server.port=8989 &>./logs.txt &"
      ],
      "metadata": {
        "id": "D1ffnFkIVBpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NGROK_API_KEY = '2wtgvNHcwOF5HkahoPPNs3m4pm7_42vXGKhZDE6LMgezWNuuU'"
      ],
      "metadata": {
        "id": "ONDn4uQiV9Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b_J-TW4ZKn6",
        "outputId": "1a9db14f-f5bb-41c9-89d1-f79bd1016854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.1.5)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set the API key directly (replace 'your_ngrok_api_key' with the actual key)\n",
        "os.environ[\"NGROK_API_KEY\"] = '2wth3NiKV8qNwTbOXV8coqnaEgB_2iPyQPFB4yERcE9YqVbAj'\n",
        "\n",
        "ngrok.set_auth_token(os.getenv(\"NGROK_API_KEY\"))"
      ],
      "metadata": {
        "id": "VUA_yqxPWnOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7643efd-3bd6-4ffd-86f3-549522fb9945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "# ngrok.kill()\n",
        "\n",
        "# ngrok.set_auth_token(userdata.get('NGROK_API_KEY'))\n",
        "\n",
        "# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
        "# ngrok_tunnel = ngrok.connect(8989)\n",
        "\n",
        "# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
        "ngrok_tunnel = ngrok.connect(8989)\n",
        "print(\"Streamlit App:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e4cfbeb-299c-4fde-c2fe-818093e08734",
        "id": "zEZiliiRVBpV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App: https://a63f-35-229-153-188.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove running app processes"
      ],
      "metadata": {
        "id": "I_k4wNzQMrcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "NJhCkMhyMrcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -ef | grep streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e8d144-08a5-431b-b183-eae41fdd649f",
        "id": "LT8xZ404MrcH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        1172     483  0 18:54 ?        00:00:00 /bin/bash -c ps -ef | grep streamlit\n",
            "root        1174    1172  0 18:54 ?        00:00:00 grep streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo kill -9 50727"
      ],
      "metadata": {
        "id": "kIjWRCybMrcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MHvNYeMsZG8-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}